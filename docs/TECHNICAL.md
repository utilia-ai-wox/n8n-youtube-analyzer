# üìö Documentation Technique

## Table des mati√®res

1. [Vue d'ensemble](#vue-densemble)
2. [Phases du workflow](#phases-du-workflow)
3. [Nodes d√©taill√©s](#nodes-d√©taill√©s)
4. [Prompts IA](#prompts-ia)
5. [Gestion des erreurs](#gestion-des-erreurs)
6. [Optimisations](#optimisations)

---

## Vue d'ensemble

### Architecture g√©n√©rale

Le workflow suit un pattern **ETL (Extract-Transform-Load)** enrichi avec de l'IA :

```
EXTRACT ‚Üí CACHE ‚Üí TRANSFORM ‚Üí ANALYZE ‚Üí GENERATE ‚Üí LOAD ‚Üí NOTIFY
```

### Flux de donn√©es

```
Input: URL YouTube
    ‚îÇ
    ‚îú‚îÄ‚îÄ video_id (11 chars)
    ‚îú‚îÄ‚îÄ metadata (titre, cha√Æne, dur√©e, vues)
    ‚îú‚îÄ‚îÄ transcript (segments + timestamps)
    ‚îÇ
    ‚ñº
Transform: Analyse IA
    ‚îÇ
    ‚îú‚îÄ‚îÄ classification (type, topics, audience)
    ‚îú‚îÄ‚îÄ summary (one-liner, executive, key_points)
    ‚îú‚îÄ‚îÄ chapters (timestamps + r√©sum√©s)
    ‚îú‚îÄ‚îÄ insights (business, tech, personal)
    ‚îú‚îÄ‚îÄ sentiment (score + tone)
    ‚îú‚îÄ‚îÄ quotes (citations m√©morables)
    ‚îú‚îÄ‚îÄ takeaways (actions concr√®tes)
    ‚îÇ
    ‚ñº
Generate: Contenu Social
    ‚îÇ
    ‚îú‚îÄ‚îÄ linkedin_post (si recommand√©)
    ‚îú‚îÄ‚îÄ twitter_thread (si recommand√©)
    ‚îú‚îÄ‚îÄ newsletter (si recommand√©)
    ‚îÇ
    ‚ñº
Output: Rapport JSON + Notion + Supabase + Slack
```

---

## Phases du workflow

### Phase 1: TRIGGER

**Nodes:** `[TRIGGER] Webhook`, `When chat message received`

Deux points d'entr√©e possibles :
- **Webhook POST** : Pour int√©gration API
- **Chat Trigger** : Pour usage interactif

### Phase 2: EXTRACT

**Nodes:** `[EXTRACT] Parse URL`, `[EXTRACT] YouTube API`, `[EXTRACT] Build Context`

1. Parse l'URL pour extraire le `video_id` (11 caract√®res)
2. Appelle YouTube Data API v3 pour les metadata
3. Construit le contexte unifi√©

### Phase 3: CACHE

**Nodes:** `[CACHE] Check Supabase`, `[CACHE] IF Exists`, `[CACHE] Return Cached`, `[CACHE] Merge Data`, `[DB] Create Entry`

1. V√©rifie si la vid√©o existe d√©j√† en base
2. Si oui ‚Üí retourne le cache (√©conomie IA)
3. Si non ‚Üí cr√©e une entr√©e vide pour marquer le traitement

### Phase 4: TRANSCR

**Nodes:** `[TRANSCR] Apify Scraper`, `[TRANSCR] Get Data`, `[TRANSCR] Adapter`

1. Lance un actor Apify pour scraper les sous-titres
2. R√©cup√®re les donn√©es brutes
3. Adapte au format unifi√© (segments + timestamps)

### Phase 5: ROUTE

**Nodes:** `[ROUTE] Check Duration`, `[ROUTE] Merge Paths`

Routage conditionnel bas√© sur la dur√©e :
- **< 30 min** : Traitement direct
- **> 30 min** : Map-Reduce (chunking)

### Phase 6: CHUNK (si vid√©o longue)

**Nodes:** `[CHUNK] Prep Map-Reduce`, `[AI] MAP - Summarize Chunk`, `[CODE] Reduce & Aggregate`

Pattern Map-Reduce :
1. **Map** : D√©coupe en chunks de ~4000 mots
2. **Process** : R√©sum√© de chaque chunk (Haiku)
3. **Reduce** : Agr√©gation des r√©sum√©s

### Phase 7: ANALYZE

**Nodes:** `[ANALYZE] AI - Summary`, `[ANALYZE] AI - Insights`, `[ANALYZE] Merge AI`, `[ANALYZE] Merge Results`

Analyse parall√®le avec Claude Sonnet :
- **Summary** : R√©sum√© + chapitres + classification + recommended_outputs
- **Insights** : Sentiment + insights + quotes + takeaways

### Phase 8: SCORE

**Nodes:** `[SCORE] Calculate Quality`

Calcul du score qualit√© (0-100) bas√© sur :
- Disponibilit√© transcript (+10 / -20)
- Volume de contenu (+10 max)
- Richesse de l'analyse (+38 max)
- Qualit√© estim√©e (+5)
- P√©nalit√©s d'erreurs

### Phase 9: GEN

**Nodes:** `[GEN] IF LinkedIn/Twitter/Newsletter`, `[GEN] AI - LinkedIn/Twitter/Newsletter`, `[GEN] NoOp`, `[GEN] Merge Content`

G√©n√©ration conditionnelle bas√©e sur `recommended_outputs` de l'IA :
- LinkedIn : Post professionnel (150-250 mots)
- Twitter : Thread viral (6-8 tweets)
- Newsletter : Digest email

### Phase 10: OUTPUT

**Nodes:** `[OUTPUT] Build Report`, `[DB] Save to Supabase`, `[NOTION] Create Page`, `[NOTION] Prepare Blocks`, `[NOTION] Add Content`, `[OUTPUT] Merge Final`, `[OUTPUT] Respond`

1. Assemble le rapport final JSON
2. Sauvegarde dans Supabase
3. Cr√©e la page Notion structur√©e
4. Ajoute les blocks de contenu
5. R√©pond au webhook

### Phase 11: NOTIFY

**Nodes:** `[NOTIFY] Slack`

Notification Slack avec :
- Titre et cha√Æne
- Score qualit√©
- Lien Notion
- Temps de traitement

### Phase 12: ERROR

**Nodes:** `[ERROR] Trigger`, `[ERROR] Format`, `[ERROR] Slack Alert`, `[ERROR] Respond`

Gestion globale des erreurs :
1. Capture automatique
2. Formatage structur√©
3. Alerte Slack
4. R√©ponse HTTP 500

---

## Nodes d√©taill√©s

### [EXTRACT] Parse URL

```javascript
/**
 * Parse une URL YouTube et extrait le video_id
 * 
 * Formats support√©s:
 * - youtube.com/watch?v=XXXXXXXXXXX
 * - youtu.be/XXXXXXXXXXX
 * - youtube.com/embed/XXXXXXXXXXX
 * - youtube.com/shorts/XXXXXXXXXXX
 * 
 * @input  {object} items[0].json - Input du trigger (body.url, url, ou chatInput)
 * @output {object} video_id, original_url, options, started_at
 */
const input = items[0].json;

// Support multiple input formats
const url = input.body?.url || input.url || input.chatInput;

if (!url) throw new Error('URL YouTube manquante');

// Patterns de matching pour video_id (11 caract√®res)
const patterns = [
  /(?:youtube\.com\/watch\?v=)([a-zA-Z0-9_-]{11})/,
  /(?:youtu\.be\/)([a-zA-Z0-9_-]{11})/,
  /(?:youtube\.com\/embed\/)([a-zA-Z0-9_-]{11})/,
  /(?:youtube\.com\/shorts\/)([a-zA-Z0-9_-]{11})/
];

let videoId = null;
for (const pattern of patterns) {
  const match = url.match(pattern);
  if (match) { videoId = match[1]; break; }
}

if (!videoId) throw new Error('URL YouTube invalide: ' + url);

return [{
  json: {
    video_id: videoId,
    original_url: url.split('&')[0], // Nettoie &t=, &list=, etc.
    session_id: input.sessionId || null,
    options: {
      language: input.body?.options?.language || 'fr',
      notify_slack: input.body?.options?.notify_slack === true
    },
    started_at: new Date().toISOString()
  }
}];
```

### [EXTRACT] Build Context

```javascript
/**
 * Construit le contexte vid√©o unifi√©
 * Combine les donn√©es de Parse URL + YouTube API
 * 
 * @input  {object} YouTube API response (snippet, contentDetails, statistics)
 * @output {object} Contexte vid√©o complet et normalis√©
 */
const urlData = $('[EXTRACT] Parse URL').first().json;
const video = items[0].json;

// Parse ISO 8601 duration (PT1H2M10S) to seconds
function parseDuration(durationStr) {
  if (!durationStr) return null;
  const match = durationStr.match(/PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?/);
  if (!match) return null;
  return (parseInt(match[1]) || 0) * 3600 + 
         (parseInt(match[2]) || 0) * 60 + 
         (parseInt(match[3]) || 0);
}

// Format seconds to HH:MM:SS or MM:SS
function formatDuration(seconds) {
  if (!seconds) return null;
  const h = Math.floor(seconds / 3600);
  const m = Math.floor((seconds % 3600) / 60);
  const s = seconds % 60;
  return h > 0 
    ? `${h}:${String(m).padStart(2,'0')}:${String(s).padStart(2,'0')}`
    : `${m}:${String(s).padStart(2,'0')}`;
}

const durationSeconds = parseDuration(video.contentDetails?.duration);

return [{
  json: {
    video_id: urlData.video_id,
    url: urlData.original_url,
    title: video.snippet?.title || 'Titre inconnu',
    channel: {
      name: video.snippet?.channelTitle || 'Cha√Æne inconnue',
      url: `https://www.youtube.com/channel/${video.snippet?.channelId}`
    },
    thumbnail: video.snippet?.thumbnails?.maxres?.url || 
               video.snippet?.thumbnails?.high?.url || 
               video.snippet?.thumbnails?.medium?.url,
    duration_seconds: durationSeconds,
    duration_formatted: formatDuration(durationSeconds),
    view_count: parseInt(video.statistics?.viewCount) || null,
    like_count: parseInt(video.statistics?.likeCount) || null,
    comment_count: parseInt(video.statistics?.commentCount) || null,
    publish_date: video.snippet?.publishedAt?.split('T')[0] || null,
    description: (video.snippet?.description || '').substring(0, 500),
    tags: video.snippet?.tags || [],
    options: urlData.options,
    started_at: urlData.started_at
  }
}];
```

### [SCORE] Calculate Quality

```javascript
/**
 * Calcule un score de qualit√© (0-100) pour l'analyse
 * 
 * Crit√®res:
 * - Disponibilit√© transcript: +10 / -20
 * - Volume de contenu: +10 max
 * - Richesse analyse: +38 max
 * - Qualit√© estim√©e: +5
 * - P√©nalit√©s erreurs: variable
 * 
 * @input  {object} Donn√©es analys√©es (chapters, insights, takeaways, etc.)
 * @output {object} Donn√©es + quality_score { score, grade, factors }
 */
const data = items[0].json;

let score = 50; // Score de base

// 1. D√âTECTION INTELLIGENTE DU CONTENU
const hasAnalysis = (data.chapters?.length > 0) || 
                   (data.summary?.executive_summary);
const hasTranscript = data.transcript?.available || 
                     (data.transcript?.word_count > 0) || 
                     hasAnalysis;

if (hasTranscript) score += 10;
else score -= 20;

// 2. VOLUME ET DENSIT√â (+10 max)
const wordCount = data.transcript?.word_count || 0;
const duration = data.video?.duration_seconds || 0;

if (wordCount > 1000 || duration > 300) score += 5;
if (wordCount > 3000 || duration > 900) score += 5;

// 3. RICHESSE DE L'ANALYSE (+38 max)
const chapterCount = data.chapters?.length || 0;
const insightCount = data.insights?.length || 0;
const takeawayCount = data.takeaways?.length || 0;
const quoteCount = data.quotes?.length || 0;

if (chapterCount >= 3) score += 5;
if (chapterCount >= 5) score += 5;
if (insightCount >= 3) score += 5;
if (insightCount >= 5) score += 5;
if (takeawayCount >= 3) score += 5;
if (quoteCount >= 2) score += 3;

// 4. QUALIT√â ESTIM√âE (+5)
const expertise = data.classification?.expertise_level || '';
if (['intermediate', 'advanced'].includes(expertise)) score += 5;

// 5. P√âNALIT√âS D'ERREURS
if (data.summary?.error) score -= 15;
if (data.sentiment?.error) score -= 10;

// Bornage 0-100
score = Math.max(0, Math.min(100, score));

// Grade A-F
const grade = score >= 90 ? 'A' : 
              score >= 75 ? 'B' : 
              score >= 60 ? 'C' : 
              score >= 40 ? 'D' : 'F';

return [{
  json: {
    ...data,
    quality_score: {
      score,
      grade,
      factors: {
        has_transcript: hasTranscript,
        transcript_length: wordCount,
        chapter_count: chapterCount,
        insight_count: insightCount,
        takeaway_count: takeawayCount
      }
    }
  }
}];
```

### [CHUNK] Prep Map-Reduce

```javascript
/**
 * Pr√©pare le Map-Reduce pour les vid√©os longues
 * D√©coupe le transcript en chunks de ~4000 mots
 * 
 * @input  {object} Donn√©es avec transcript.segments
 * @output {array}  Multiple items (1 par chunk) pour traitement parall√®le
 */
const input = items[0].json;
const segments = input.transcript.segments || [];
const CHUNK_SIZE = 4000; // mots par chunk

let chunks = [];
let currentChunk = [];
let currentWordCount = 0;

// 1. D√©coupage intelligent
for (const seg of segments) {
  const segWords = seg.text.split(/\s+/).length;
  
  if (currentWordCount + segWords > CHUNK_SIZE && currentChunk.length > 0) {
    chunks.push({
      text: currentChunk.map(s => s.text).join(' '),
      start: currentChunk[0].start_formatted,
      end: seg.start_formatted
    });
    currentChunk = [];
    currentWordCount = 0;
  }
  
  currentChunk.push(seg);
  currentWordCount += segWords;
}

// Dernier chunk
if (currentChunk.length > 0) {
  chunks.push({
    text: currentChunk.map(s => s.text).join(' '),
    start: currentChunk[0].start_formatted,
    end: segments[segments.length-1].start_formatted
  });
}

// 2. Retourne MULTIPLE ITEMS (n8n ex√©cute le node suivant pour chaque)
return chunks.map((chunk, index) => ({
  json: {
    ...input,
    processing_mode: 'chunked',
    chunk_index: index + 1,
    total_chunks: chunks.length,
    chunk_content: chunk.text,
    chunk_start: chunk.start,
    chunk_end: chunk.end,
    transcript: { ...input.transcript, full_text: '' } // All√®ge la m√©moire
  }
}));
```

---

## Prompts IA

### [ANALYZE] AI - Summary

**Temp√©rature:** 0.3 | **Max Tokens:** 4096

```
Tu es un expert en analyse de contenu vid√©o. Tu produis des analyses structur√©es, pr√©cises et actionnables.

G√âN√àRE UN JSON VALIDE avec cette structure:
{
  "summary": {
    "one_liner": "R√©sum√© en une phrase percutante (max 120 caract√®res)",
    "executive_summary": "R√©sum√© ex√©cutif en 3-5 phrases pour un d√©cideur press√©",
    "key_points": ["Point cl√© 1", "Point cl√© 2", "Point cl√© 3", "Point cl√© 4", "Point cl√© 5"],
    "tldr": "Version ultra-courte en 1-2 phrases"
  },
  "chapters": [
    {
      "title": "Titre du chapitre",
      "timestamp": "MM:SS",
      "duration_estimate": "X min",
      "summary": "Ce qui est couvert",
      "key_takeaway": "Le point le plus important"
    }
  ],
  "structure": {
    "has_intro": true,
    "has_conclusion": true,
    "main_sections": 5,
    "pacing": "slow|medium|fast"
  },
  "classification": {
    "content_type": "tutorial|interview|review|news|educational|entertainment|podcast|vlog",
    "primary_topic": "Sujet principal",
    "secondary_topics": ["topic1", "topic2"],
    "target_audience": "Description de l'audience cible",
    "expertise_level": "beginner|intermediate|advanced"
  },
  "recommended_outputs": {
    "linkedin": true,
    "twitter": true,
    "newsletter": false,
    "reasoning": "Explication courte du choix"
  }
}

R√àGLES CLASSIFICATION:
- linkedin: true si contenu professionnel, business, √©ducatif, tech
- twitter: true si insights rapides, quotable, news, opinions
- newsletter: true si contenu dense, long format, analyse approfondie

R√àGLES G√âN√âRALES:
- 5-8 chapitres maximum
- 5-7 points cl√©s
- Timestamps au format MM:SS
- JSON VALIDE UNIQUEMENT
```

### [ANALYZE] AI - Insights

**Temp√©rature:** 0.2 | **Max Tokens:** 3000

```
Tu es un expert en extraction d'insights business et d√©veloppement personnel.

G√âN√àRE UN JSON VALIDE:
{
  "sentiment": {
    "overall": "positive|negative|neutral|mixed",
    "score": 0.75,
    "confidence": 0.9,
    "tone": ["informatif", "enthousiaste", "critique"]
  },
  "insights": [
    {
      "insight": "L'insight actionnable",
      "context": "Le contexte qui le supporte",
      "category": "business|tech|personal|strategy|trend|mindset",
      "importance": "high|medium|low",
      "applicable_to": "Qui peut appliquer cet insight"
    }
  ],
  "quotes": [
    {
      "quote": "Citation m√©morable",
      "speaker": "Qui l'a dit",
      "significance": "Pourquoi c'est important",
      "tweetable": true
    }
  ],
  "actionable_takeaways": [
    {
      "action": "Action concr√®te √† faire",
      "effort": "low|medium|high",
      "impact": "low|medium|high",
      "timeframe": "immediate|short-term|long-term"
    }
  ],
  "contrarian_views": ["Point de vue contre-intuitif 1"],
  "resources_mentioned": [
    {
      "name": "Nom de la ressource",
      "type": "book|tool|website|person",
      "context": "Comment elle a √©t√© mentionn√©e"
    }
  ]
}

R√àGLES:
- 5-8 insights class√©s par importance
- 2-4 citations m√©morables
- 3-5 takeaways actionnables
- JSON VALIDE UNIQUEMENT
```

### [GEN] AI - LinkedIn

**Temp√©rature:** 0.7 | **Max Tokens:** 2000

```
Tu es un expert LinkedIn avec 100k+ followers. Cr√©e un post viral.

STRUCTURE:
1. HOOK (1√®re ligne) - DOIT captiver en 2 secondes avec emoji
2. CORPS - 3-5 points avec emojis, insights transform√©s en valeur
3. CTA - Question engageante pour les commentaires
4. HASHTAGS - 4-5 pertinents et populaires

FORMAT:
- 150-250 mots total
- Sauts de ligne fr√©quents (mobile-first)
- Chaque ligne = 1 id√©e max
- Emojis strat√©giques
- Ton professionnel mais humain

JSON:
{
  "linkedin_post": {
    "hook": "Premi√®re ligne d'accroche",
    "body": "Corps du post",
    "cta": "Question engageante",
    "hashtags": ["#tag1", "#tag2"],
    "full_post": "Post complet pr√™t √† copier"
  }
}
```

### [GEN] AI - Twitter

**Temp√©rature:** 0.7 | **Max Tokens:** 2000

```
Tu es un expert Twitter/X avec des threads viraux. Cr√©e un thread engageant.

STRUCTURE:
- Tweet 1: üßµ Hook puissant + annonce du thread
- Tweets 2-6: Un insight par tweet, num√©rot√©s (1/, 2/, etc.)
- Tweet final: R√©cap + CTA + lien vid√©o

R√àGLES STRICTES:
- MAX 280 caract√®res par tweet (OBLIGATOIRE)
- 6-8 tweets total
- Chaque tweet = valeur standalone
- Emojis au d√©but

JSON:
{
  "twitter_thread": {
    "tweets": [
      {"number": 1, "content": "üßµ Hook...", "char_count": 145}
    ],
    "total_tweets": 7
  }
}
```

---

## Gestion des erreurs

### Pattern Error Handling

```
[Tout node]
    ‚îÇ
    ‚îî‚îÄ‚îÄ Error ‚Üí [ERROR] Trigger
                    ‚îÇ
                    ‚ñº
              [ERROR] Format
                    ‚îÇ
                    ‚ñº
              [ERROR] Slack Alert
                    ‚îÇ
                    ‚ñº
              [ERROR] Respond (500)
```

### Structure d'erreur

```json
{
  "error": true,
  "message": "Description de l'erreur",
  "node": "Nom du node fautif",
  "execution_id": "abc123",
  "timestamp": "2025-12-07T10:00:00.000Z",
  "workflow": "YouTube Content Analyzer Pro",
  "stack": "Stack trace (500 premiers chars)"
}
```

---

## Optimisations

### Performance

1. **Cache Supabase** : √âvite de retraiter une vid√©o d√©j√† analys√©e
2. **Map-Reduce** : Parall√©lise le traitement des vid√©os longues
3. **Haiku pour chunks** : Mod√®le plus rapide et moins cher pour les r√©sum√©s interm√©diaires
4. **NoOp nodes** : Skip propre des branches non utilis√©es

### Co√ªts

1. **IA d√©cisionnelle** : Ne g√©n√®re que le contenu pertinent
2. **Chunking intelligent** : Limite la taille des prompts
3. **Cache** : √âconomise 100% des co√ªts sur les vid√©os d√©j√† trait√©es

### Fiabilit√©

1. **Error handling global** : Capture toutes les erreurs
2. **Alertes Slack** : Notification imm√©diate des probl√®mes
3. **Optional chaining** : `?.` partout pour √©viter les undefined
4. **Fallbacks** : Valeurs par d√©faut syst√©matiques
